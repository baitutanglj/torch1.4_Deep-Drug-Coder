{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "ddc_env_final",
   "display_name": "ddc_env_final"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Occupy a GPU for the model to be loaded \n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "# GPU ID, if occupied change to an available GPU ID listed under !nvidia-smi\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import numpy as np\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "import h5py, ast, pickle\n",
    "\n",
    "from ddc_pub import ddc_v3 as ddc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors(binmols_list, qsar_model=None):\n",
    "    \"\"\"Calculate molecular descriptors of SMILES in a list.\n",
    "    The descriptors are logp, tpsa, mw, qed, hba, hbd and probability of being active towards DRD2.\n",
    "    \n",
    "    Returns:\n",
    "        A np.ndarray of descriptors.\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "    import rdkit\n",
    "    from rdkit import Chem, DataStructs\n",
    "    from rdkit.Chem import Descriptors, rdMolDescriptors, AllChem, QED\n",
    "    \n",
    "    descriptors = []\n",
    "    active_mols = []\n",
    "    \n",
    "    for idx, binmol in enumerate(binmols_list):\n",
    "        mol = Chem.Mol(binmol)\n",
    "        if mol:\n",
    "            try:\n",
    "                logp  = Descriptors.MolLogP(mol)\n",
    "                tpsa  = Descriptors.TPSA(mol)\n",
    "                molwt = Descriptors.ExactMolWt(mol)\n",
    "                hba   = rdMolDescriptors.CalcNumHBA(mol)\n",
    "                hbd   = rdMolDescriptors.CalcNumHBD(mol)\n",
    "                qed   = QED.qed(mol)\n",
    "                \n",
    "                fp = AllChem.GetMorganFingerprintAsBitVect(mol,2, nBits=2048)\n",
    "                ecfp4 = np.zeros((2048,))\n",
    "                DataStructs.ConvertToNumpyArray(fp, ecfp4) \n",
    "                active = qsar_model.predict_proba([ecfp4])[0][1]\n",
    "                descriptors.append([logp, tpsa, molwt, qed, hba, hbd, active]) \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"Invalid generation.\")\n",
    "            \n",
    "    return np.asarray(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QSAR model\n",
    "qsar_model_name = \"models/qsar_model.pickle\"\n",
    "with open(qsar_model_name, \"rb\") as file:\n",
    "    qsar_model = pickle.load(file)[\"classifier_sv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_filename = \"datasets/CHEMBL25_TRAIN_MOLS.h5\"\n",
    "with h5py.File(dataset_filename, \"r\") as f:\n",
    "    binmols = f[\"mols\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the descriptors for the molecules in the dataset\n",
    "# This process takes a lot of time and it's good if the descriptors are\n",
    "# pre-calculated and stored in a file to load every time\n",
    "descr = get_descriptors(binmols, qsar_model=qsar_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All apriori known characters of the SMILES in the dataset\n",
    "charset = \"Brc1(-23[nH])45C=NOso#FlS67+89%0\"\n",
    "# Apriori known max length of the SMILES in the dataset\n",
    "maxlen = 128\n",
    "# Name of the dataset\n",
    "name = \"ChEMBL25_TRAIN\"\n",
    "\n",
    "dataset_info = {\"charset\": charset, \"maxlen\": maxlen, \"name\": name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a model\n",
    "model = ddc.DDC(x              = descr,        # input\n",
    "                y              = binmols,      # output\n",
    "                dataset_info   = dataset_info, # dataset information\n",
    "                scaling        = True,         # scale the descriptors\n",
    "                noise_std      = 0.1,          # std of the noise layer\n",
    "                lstm_dim       = 512,          # breadth of LSTM layers\n",
    "                dec_layers     = 3,            # number of decoding layers\n",
    "                batch_size     = 128)          # batch size for training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(epochs              = 100,         # number of epochs\n",
    "          lr                  = 1e-3,        # initial learning rate for Adam, recommended\n",
    "          model_name          = \"new_model\", # base name to append the checkpoints with\n",
    "          checkpoint_dir      = \"\",          # save checkpoints in the notebook's directory\n",
    "          mini_epochs         = 10,          # number of sub-epochs within an epoch to trigger lr decay\n",
    "          save_period         = 50,          # checkpoint frequency (in mini_epochs)\n",
    "          lr_decay            = True,        # whether to use exponential lr decay or not\n",
    "          sch_epoch_to_start  = 500,         # mini-epoch to start lr decay (bypassed if lr_decay=False)\n",
    "          sch_lr_init         = 1e-3,        # initial lr, should be equal to lr (bypassed if lr_decay=False)\n",
    "          sch_lr_final        = 1e-6,        # final lr before finishing training (bypassed if lr_decay=False)\n",
    "          patience            = 25)          # patience for Keras' ReduceLROnPlateau (bypassed if lr_decay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save(\"new_model\")"
   ]
  }
 ]
}