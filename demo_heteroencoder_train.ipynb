{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Occupy a GPU for the model to be loaded \n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "# GPU ID, if occupied change to an available GPU ID listed under !nvidia-smi\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import numpy as np\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "import h5py, ast, pickle\n",
    "\n",
    "from ddc_pub import ddc_v3 as ddc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_filename = \"datasets/CHEMBL25_TRAIN_MOLS.h5\"\n",
    "with h5py.File(dataset_filename, \"r\") as f:\n",
    "    binmols = f[\"mols\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All apriori known characters of the SMILES in the dataset\n",
    "charset = \"Brc1(-23[nH])45C=NOso#FlS67+89%0\"\n",
    "# Apriori known max length of the SMILES in the dataset\n",
    "maxlen = 128\n",
    "# Name of the dataset\n",
    "name = \"ChEMBL25_TRAIN\"\n",
    "\n",
    "dataset_info = {\"charset\": charset, \"maxlen\": maxlen, \"name\": name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a model\n",
    "model = ddc.DDC(x              = binmols,      # input\n",
    "                y              = binmols,      # output\n",
    "                dataset_info   = dataset_info, # dataset information\n",
    "                noise_std      = 0.1,          # std of the noise layer\n",
    "                lstm_dim       = 512,          # breadth of LSTM layers\n",
    "                dec_layers     = 3,            # number of decoding layers\n",
    "                codelayer_dim  = 128,          # dimensionality of latent space\n",
    "                batch_size     = 128)          # batch size for training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(epochs              = 100,         # number of epochs\n",
    "          lr                  = 1e-3,        # initial learning rate for Adam, recommended\n",
    "          model_name          = \"new_model\", # base name to append the checkpoints with\n",
    "          checkpoint_dir      = \"\",          # save checkpoints in the notebook's directory\n",
    "          mini_epochs         = 10,          # number of sub-epochs within an epoch to trigger lr decay\n",
    "          save_period         = 50,          # checkpoint frequency (in mini_epochs)\n",
    "          lr_decay            = True,        # whether to use exponential lr decay or not\n",
    "          sch_epoch_to_start  = 500,         # mini-epoch to start lr decay (bypassed if lr_decay=False)\n",
    "          sch_lr_init         = 1e-3,        # initial lr, should be equal to lr (bypassed if lr_decay=False)\n",
    "          sch_lr_final        = 1e-6,        # final lr before finishing training (bypassed if lr_decay=False)\n",
    "          patience            = 25)          # patience for Keras' ReduceLROnPlateau (bypassed if lr_decay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save(\"new_model\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "version": "3.6.8-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
